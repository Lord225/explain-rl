{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 09:04:45.410364: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-07 09:04:45.468226: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-07 09:04:45.468266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-07 09:04:45.470704: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-07 09:04:45.482684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-07 09:04:46.482893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building procgen...done\n",
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35 # 1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "- Python: 3.9.21\n",
      "- Stable-Baselines3: 2.5.0\n",
      "- PyTorch: 2.6.0+cu124\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 3.1.1\n",
      "- Gymnasium: 1.0.0\n",
      "- OpenAI Gym: 0.26.2\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39 # 1 SMP Tue Nov 5 00:21:55 UTC 2024\n",
      "- Python: 3.9.21\n",
      "- Stable-Baselines3: 2.5.0\n",
      "- PyTorch: 2.6.0+cu124\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 1.2.2\n",
      "- Gymnasium: 1.0.0\n",
      "- OpenAI Gym: 0.15.7\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "(10000, 64, 64, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (to_patch_embedding): Sequential(\n",
       "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n",
       "    (1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Linear(in_features=144, out_features=64, bias=True)\n",
       "    (3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (transformer): Transformer(\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x ModuleList(\n",
       "        (0): Attention(\n",
       "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (attend): Softmax(dim=-1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_qkv): Linear(in_features=64, out_features=1152, bias=False)\n",
       "          (to_out): Sequential(\n",
       "            (0): Linear(in_features=384, out_features=64, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (2): GELU(approximate='none')\n",
       "            (3): Dropout(p=0.0, inplace=False)\n",
       "            (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (5): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_latent): Identity()\n",
       "  (mlp_head): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "sys.path.append(\"/home/lord225/pyrepos/explain-rl\")\n",
    "\n",
    "from ppo import PPO\n",
    "import procgenwrapper\n",
    "\n",
    "MODEL_PATH = \"/home/lord225/pyrepos/explain-rl/preserve\"\n",
    "DATA_PATH = \"/home/lord225/pyrepos/explain-rl/explain/records\"\n",
    "\n",
    "MODEL_NAME = \"20250401-154655-AudienceYourYes_51_v3.1\"\n",
    "\n",
    "MODEL_PATH = os.path.join(MODEL_PATH, MODEL_NAME)\n",
    "DATA_PATH = os.path.join(DATA_PATH, MODEL_NAME+\"_replay.h5\")\n",
    "\n",
    "venv = procgenwrapper.ProcGenWrapper(\"starpilot\", human=False, collect_seg=True)\n",
    "\n",
    "model = PPO.load(MODEL_PATH,\n",
    "                    env=venv, \n",
    "                    print_system_info=True,\n",
    "                    custom_objects={\n",
    "                        \"ViT\":ViT,\n",
    "                    })\n",
    "\n",
    "dataset = h5py.File(DATA_PATH, \"r\")\n",
    "\n",
    "observations = np.array(dataset[\"observations\"])\n",
    "actions = np.array(dataset[\"actions\"])\n",
    "rewards = np.array(dataset[\"rewards\"])\n",
    "dones = np.array(dataset[\"dones\"])\n",
    "seg_observations = np.array(dataset[\"seg_observations\"])\n",
    "next_observations = np.array(dataset[\"next_observations\"])\n",
    "\n",
    "dataset.close() \n",
    "\n",
    "print(observations.shape)\n",
    "\n",
    "net = model.policy\n",
    "vit = net.mlp_extractor.policy_net[1]\n",
    "vit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_img(obs):\n",
    "    return obs[:, :, 6:9]\n",
    "\n",
    "ax, fig = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "fig[0].imshow(get_img(observations[120]))\n",
    "fig[1].imshow(seg_observations[120], vmin=0, vmax=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64]) torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "# Reshape the observations to match the expected input shape\n",
    "obs_tensor = th.tensor(observations[:1].reshape((1, -1)), dtype=th.float32, device=net.device)\n",
    "\n",
    "# Pass the tensor through the action_net\n",
    "features_a, features_b = net.mlp_extractor(obs_tensor)\n",
    "\n",
    "print(features_a.shape, features_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15])\n"
     ]
    }
   ],
   "source": [
    "# Get the action logits\n",
    "action_logits = net.action_net(features_a)\n",
    "\n",
    "print(action_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.extractor import Extractor\n",
    "from vit_pytorch.recorder import Recorder\n",
    "\n",
    "extractor = Extractor(vit).cpu()\n",
    "recorder = Recorder(vit).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([1, 257, 64]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas, extracted = extractor(th.tensor(observations[:1]).permute(0, 3, 1, 2).float().cpu())\n",
    "\n",
    "probas.shape, extracted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([1, 4, 6, 257, 257]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas, attention = recorder(th.tensor(observations[:1]).permute(0, 3, 1, 2).float().cpu())\n",
    "\n",
    "probas.shape, attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def attention_map(attention, image):\n",
    "    # avg attention over heads\n",
    "    attention = np.array(attention).mean(axis=1)\n",
    "\n",
    "    grid_size = int(np.sqrt(attention.shape[-1] - 1))\n",
    "    num_layers = attention.shape[0]\n",
    "    num_heads = attention.shape[1]\n",
    "    reshaped = attention.reshape(\n",
    "        (num_layers, num_heads, grid_size**2 + 1, grid_size**2 + 1)\n",
    "    )\n",
    "\n",
    "    # From Appendix D.6 in the paper ...\n",
    "    # Average the attention weights across all heads.\n",
    "    reshaped = reshaped.mean(axis=1)\n",
    "\n",
    "    # From Section 3 in https://arxiv.org/pdf/2005.00928.pdf ...\n",
    "    # To account for residual connections, we add an identity matrix to the\n",
    "    # attention matrix and re-normalize the weights.\n",
    "    reshaped = reshaped + np.eye(reshaped.shape[1])\n",
    "    reshaped = reshaped / reshaped.sum(axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
    "\n",
    "    # Recursively multiply the weight matrices\n",
    "    v = reshaped[-1]\n",
    "    for n in range(1, len(reshaped)):\n",
    "        v = np.matmul(v, reshaped[-1 - n])\n",
    "\n",
    "    # Attention from the output token to the input space.\n",
    "    mask = v[0, 1:].reshape(grid_size, grid_size)\n",
    "    mask = mask / mask.max()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recorder = recorder.cpu()\n",
    "frame = 304\n",
    "\n",
    "obs = th.tensor(observations[frame]).reshape(-1, 64, 64, 9).permute(0, 3, 1, 2).float().cpu()\n",
    "\n",
    "probas, attention = recorder(obs)\n",
    "\n",
    "map = attention_map(attention.cpu().detach().numpy(), observations[frame])\n",
    "\n",
    "# plot both\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(get_img(observations[frame]))\n",
    "ax[1].imshow(map)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def generate_attention_gif(observations, filename='attention.gif', duration=0.1):\n",
    "    frames = []\n",
    "    for i in tqdm.tqdm(range(len(observations))):\n",
    "        obs = th.tensor(observations[i]).reshape(-1, 64, 64, 9).permute(0, 3, 1, 2).float().cpu()\n",
    "        probas, attention = recorder(obs)\n",
    "        map = attention_map(attention.cpu().detach().numpy(), observations[i])\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax[0].imshow(get_img(observations[i]))\n",
    "        ax[1].imshow(map)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        fig.canvas.draw()\n",
    "        image = np.frombuffer(fig.canvas.buffer_rgba(), dtype='uint8')\n",
    "        image = image.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "        frames.append(image)\n",
    "    # Save the frames as a gif\n",
    "    imageio.mimsave(filename, frames, duration=duration, loop=0)\n",
    "    \n",
    "\n",
    "filename = f\"attention-{MODEL_NAME}.gif\"\n",
    "if not os.path.exists(filename):\n",
    "    generate_attention_gif(observations[:300], filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = 5_000\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 2375/5000 [00:23<00:25, 102.75it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 14.34 GiB is allocated by PyTorch, and 3.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, DATASET_SIZE, batch_size)):\n\u001b[1;32m      6\u001b[0m     batch \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor(observations[i:i\u001b[38;5;241m+\u001b[39mbatch_size], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 7\u001b[0m     probs, features \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     features_list\u001b[38;5;241m.\u001b[39mappend(features\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m batch\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/vit_pytorch/extractor.py:82\u001b[0m, in \u001b[0;36mExtractor.forward\u001b[0;34m(self, img, return_embeddings_only)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_registered:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_hook()\n\u001b[0;32m---> 82\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m target_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01melse\u001b[39;00m img\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     85\u001b[0m latents \u001b[38;5;241m=\u001b[39m apply_tuple_or_single(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mto(target_device), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatents)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/vit_pytorch/vit.py:122\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    119\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding[:, :(n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    120\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m--> 122\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_latent(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/vit_pytorch/vit.py:78\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attn, ff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 78\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     79\u001b[0m         x \u001b[38;5;241m=\u001b[39m ff(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/vit_pytorch/vit.py:58\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: rearrange(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m'\u001b[39m, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads), qkv)\n\u001b[1;32m     56\u001b[0m dots \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[0;32m---> 58\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdots\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn)\n\u001b[1;32m     61\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn, v)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1806\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1804\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1806\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1809\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/vit_pytorch/recorder.py:23\u001b[0m, in \u001b[0;36mRecorder._hook\u001b[0;34m(self, _, input, output)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_hook\u001b[39m(\u001b[38;5;28mself\u001b[39m, _, \u001b[38;5;28minput\u001b[39m, output):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecordings\u001b[38;5;241m.\u001b[39mappend(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach())\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 14.34 GiB is allocated by PyTorch, and 3.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# calculate features using extractor\n",
    "extractor = extractor.cuda()\n",
    "features_list = []\n",
    "\n",
    "for i in tqdm.tqdm(range(0, DATASET_SIZE, batch_size)):\n",
    "    batch = th.tensor(observations[i:i+batch_size], device=\"cpu\").permute(0, 3, 1, 2).float().cuda()\n",
    "    probs, features = extractor(batch)\n",
    "    features_list.append(features.cpu().detach().numpy())\n",
    "\n",
    "    del batch\n",
    "    del features\n",
    "\n",
    "features_list = np.array(features_list).squeeze()\n",
    "\n",
    "features_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "features_list.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
